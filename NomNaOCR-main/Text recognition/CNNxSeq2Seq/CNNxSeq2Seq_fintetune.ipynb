{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3307,
     "status": "ok",
     "timestamp": 1650387167424,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "670dNtc0LR0z"
   },
   "outputs": [],
   "source": [
    "# https://github.com/bai-shang/crnn_seq2seq_ocr_pytorch\n",
    "# https://github.com/chenjun2hao/Attention_ocr.pytorch\n",
    "# https://github.com/alleveenstra/attentionocr\n",
    "# https://github.com/koibiki/CRNN-ATTENTION\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "APPROACH_NAME = 'CNNxSeq2Seq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL95XLRnDLr0"
   },
   "source": [
    "# Check GPU working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650387168085,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "QC47NIx6C8tx",
    "outputId": "1e78186e-e968-489e-a488-bd103f9f919c"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0': raise SystemError('GPU device not found')\n",
    "print('Found GPU at:', device_name)\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0RvhBn63BXh"
   },
   "source": [
    "# Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1650387168752,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "i7sM_I-v3BXi"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = r'../../Datasets/Patches'\n",
    "ALL_TRANSCRIPTS_PATH = f'{DATASET_DIR}/All.txt'\n",
    "VALID_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate.txt'\n",
    "FONT_PATH = r'../../NomNaTong-Regular.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9lAUxB63BXj"
   },
   "source": [
    "## Load and remove records with rare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2137,
     "status": "ok",
     "timestamp": 1650387170888,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "jYzNVfQU18u7",
    "outputId": "4633b82d-7f6b-47f7-e526-be508cbef0a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from loader import DataImporter\n",
    "dataset = DataImporter(DATASET_DIR, ALL_TRANSCRIPTS_PATH, min_length=1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ufvmp3v18u7"
   },
   "source": [
    "## Data constants and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1650387170888,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "7EvxKf_W18u8"
   },
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 432, 48\n",
    "PADDING_CHAR = '[PAD]' \n",
    "START_CHAR = '[START]'\n",
    "END_CHAR = '[END]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1650387170889,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "buqQdwSa18u8"
   },
   "outputs": [],
   "source": [
    "from loader import DataHandler\n",
    "data_handler = DataHandler(\n",
    "    dataset, \n",
    "    img_size = (HEIGHT, WIDTH), \n",
    "    padding_char = PADDING_CHAR,\n",
    "    start_char = START_CHAR,\n",
    "    end_char = END_CHAR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1650387170889,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "bN5vVmTP18u9"
   },
   "outputs": [],
   "source": [
    "NUM_VALIDATE = DataImporter(DATASET_DIR, VALID_TRANSCRIPTS_PATH, min_length=1).size\n",
    "START_TOKEN = data_handler.start_token\n",
    "END_TOKEN = data_handler.end_token\n",
    "VOCAB_SIZE = data_handler.char2num.vocab_size()\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqe2peRS3BXr"
   },
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 2044,
     "status": "ok",
     "timestamp": 1650387172930,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "S_vABuOD3BXs",
    "outputId": "82254188-5309-4dd8-bb4f-363a73606aba"
   },
   "outputs": [],
   "source": [
    "from visualizer import visualize_images_labels\n",
    "visualize_images_labels(\n",
    "    dataset.img_paths, \n",
    "    dataset.labels, \n",
    "    figsize = (15, 15),\n",
    "    subplot_size = (2, 8),\n",
    "    font_path = FONT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQgAf-bd3BX0"
   },
   "source": [
    "# Define model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650387172930,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "Yu_6hrxZ3BX0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, GRU, \n",
    "    Bidirectional, Concatenate, Flatten\n",
    ")\n",
    "from layers import custom_cnn, reshape_features, AdditiveAttention\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGD7dMxt3BX1"
   },
   "source": [
    "## The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650387172931,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "WRfgDRNZ3BX2"
   },
   "outputs": [],
   "source": [
    "def Encoder(imagenet_model=None, imagenet_output_layer=None, name='Encoder'):\n",
    "    if imagenet_model: # Use Imagenet model as CNN layers\n",
    "        image_input = imagenet_model.input\n",
    "        imagenet_model.layers[0]._name = 'image'\n",
    "        x = imagenet_model.get_layer(imagenet_output_layer).output\n",
    "    else: \n",
    "        image_input = Input(shape=(HEIGHT, WIDTH, 3), dtype='float32', name='image')\n",
    "        conv_blocks_config = {\n",
    "            'block1': {'num_conv': 1, 'filters':  64, 'pool_size': (2, 2)}, \n",
    "            'block2': {'num_conv': 1, 'filters': 128, 'pool_size': (2, 2)}, \n",
    "            'block3': {'num_conv': 2, 'filters': 256, 'pool_size': (2, 2)}, \n",
    "            'block4': {'num_conv': 2, 'filters': 512, 'pool_size': (2, 2)}, \n",
    "            \n",
    "            # Last Conv blocks with 2x2 kernel but without no padding and pooling layer\n",
    "            'block5': {'num_conv': 2, 'filters': 512, 'pool_size': None}, \n",
    "        }\n",
    "        x = custom_cnn(conv_blocks_config, image_input)\n",
    "\n",
    "    # Reshape accordingly before passing output to RNN\n",
    "    feature_maps = reshape_features(x, dim_to_keep=1, name='rnn_input')\n",
    "    \n",
    "    # RNN layers\n",
    "    bigru1 = Bidirectional(GRU(UNITS, return_sequences=True), name='bigru1')(feature_maps)\n",
    "    bigru2, forward_state, backward_state = Bidirectional(GRU(\n",
    "        units = UNITS, \n",
    "        return_sequences = True,\n",
    "        return_state = True\n",
    "    ), name = 'bigru2')(bigru1)\n",
    "    \n",
    "    # Concat states of 2 directions\n",
    "    final_state = Concatenate(name='encoder_state')([forward_state, backward_state])\n",
    "    return tf.keras.Model(inputs=image_input, outputs=[bigru2, final_state], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8TI5f473BX5"
   },
   "source": [
    "## The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(enc_seq_shape, name='Decoder'):\n",
    "    token_input = Input(shape=(1,), name='new_token')\n",
    "    enc_seq_input = Input(shape=enc_seq_shape, name='encoder_sequence')\n",
    "    \n",
    "    # In encoder we used Bidirectional so we have to take double UNITS  \n",
    "    # for single decoder GRU using encoder's final state as initial state\n",
    "    dec_units = UNITS * 2\n",
    "    pre_hidden_input = Input(shape=(dec_units,), name='previous_state')\n",
    "    \n",
    "    # Process one step with the RNN\n",
    "    x = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(token_input)\n",
    "    rnn_output, state = GRU(\n",
    "        units = dec_units, \n",
    "        return_state = True, \n",
    "        return_sequences = True,\n",
    "        name = 'dec_gru'\n",
    "    )(x, initial_state=pre_hidden_input)\n",
    "    \n",
    "    # Use the RNN output as the query for the attention over the encoder output\n",
    "    attention = AdditiveAttention(dec_units)\n",
    "    context_vector, attention_weights = attention(rnn_output, enc_seq_input)\n",
    "    \n",
    "    # Form the attention vector\n",
    "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "    attention_vector = Dense(\n",
    "        units = dec_units, \n",
    "        activation = 'tanh', \n",
    "        use_bias = False,\n",
    "        name = 'attention_vector'\n",
    "    )(context_and_rnn_output)\n",
    "    \n",
    "    # Generate predictions\n",
    "    x = Flatten()(attention_vector)\n",
    "    y_pred = Dense(VOCAB_SIZE, name='prediction')(x)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs = [token_input, enc_seq_input, pre_hidden_input], \n",
    "        outputs = [y_pred, state, attention_weights],\n",
    "        name = name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2yRAV_b18u_"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1650387172931,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "XZFllx_l3BX1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import get_imagenet_model, EncoderDecoderModel\n",
    "imagenet_model, imagenet_output_layer = None, None\n",
    "# # Pick a model from https://keras.io/api/applications\n",
    "# imagenet_model = get_imagenet_model('VGG16', (HEIGHT, WIDTH, 3))\n",
    "# imagenet_output_layer = 'block5_pool'\n",
    "# imagenet_model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1650387173631,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "rE8cvG9T3BX9"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(imagenet_model, imagenet_output_layer)\n",
    "decoder = Decoder(encoder.outputs[0].shape[1:])\n",
    "encoder.load_weights(f'./IHR-NomDB/IHR-NomDB_{APPROACH_NAME}_enc.h5')\n",
    "decoder.load_weights(\n",
    "    f'./IHR-NomDB/IHR-NomDB_{APPROACH_NAME}_dec.h5', \n",
    "    skip_mismatch = True,\n",
    "    by_name = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650387173632,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "Q6IfTtNF18vB",
    "outputId": "f978aa70-8b06-49f3-e8d7-e10ee7fa1e69",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel(encoder, decoder, data_handler)\n",
    "encoder.summary(line_length=120)\n",
    "print()\n",
    "decoder.summary(line_length=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjT21l-13BX-"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650387173632,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "EMOpiKBb18vB"
   },
   "outputs": [],
   "source": [
    "train_idxs = list(range(dataset.size - NUM_VALIDATE))\n",
    "valid_idxs = list(range(train_idxs[-1] + 1, dataset.size))\n",
    "print('Number of training samples:', len(train_idxs))\n",
    "print('Number of validate samples:', len(valid_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2022)\n",
    "random.shuffle(train_idxs)\n",
    "\n",
    "# When run on a small RAM machine, you can set use_cache=False to \n",
    "# not run out of memory but it will slow down the training speed\n",
    "train_tf_dataset = data_handler.prepare_tf_dataset(train_idxs, BATCH_SIZE, drop_remainder=True)\n",
    "valid_tf_dataset = data_handler.prepare_tf_dataset(valid_idxs, BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baKJLLO6PGWV"
   },
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650387174197,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "J98bxb_PPEOb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 1e-3, # Change that less than 1e-3, will count as no improvement\n",
    "    patience = 5, # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights = True, # Restore weights from the epoch with the best value\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# Reduce the learning rate once learning stagnates\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    patience = 2, # Reduce if no improvement after 2 epochs\n",
    "    min_lr = 1e-6, # Lower bound on the learning rate \n",
    "    factor = 0.5, # => new_lr = lr * factor\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sh1CdC18vB"
   },
   "source": [
    "## Fine-tuning the NomNaOCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650387174197,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "1HGg9k1t3BX-"
   },
   "outputs": [],
   "source": [
    "from losses import MaskedLoss\n",
    "from metrics import SequenceAccuracy, CharacterAccuracy, LevenshteinDistance\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "LEARNING_RATE = 2e-4\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650387174197,
     "user": {
      "displayName": "Quan Dang Hoang",
      "userId": "14358170870463268921"
     },
     "user_tz": -420
    },
    "id": "CPVb3axr18vC"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(LEARNING_RATE), \n",
    "    loss = MaskedLoss(), \n",
    "    metrics = [\n",
    "        SequenceAccuracy(),\n",
    "        CharacterAccuracy(),\n",
    "        LevenshteinDistance(normalize=True, name='lev_distance')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiWc6i5B3BYB",
    "outputId": "b53c90fa-bebd-4ab7-8ff3-a5ca5f8d35b8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data = valid_tf_dataset,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [reduce_lr_callback, early_stopping_callback],\n",
    "    verbose = 1\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jU-5S6vo18vC"
   },
   "source": [
    "## Save the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtA0QVnz18vC"
   },
   "outputs": [],
   "source": [
    "best_epoch = early_stopping_callback.best_epoch\n",
    "print(f'- Loss on validation\\t: {history[\"val_loss\"][best_epoch]}')\n",
    "print(f'- Sequence accuracy\\t: {history[\"val_seq_acc\"][best_epoch]}')\n",
    "print(f'- Character accuracy\\t: {history[\"val_char_acc\"][best_epoch]}')\n",
    "print(f'- Levenshtein distance\\t: {history[\"val_lev_distance\"][best_epoch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5MgFd3a18vC"
   },
   "outputs": [],
   "source": [
    "from visualizer import plot_training_results\n",
    "plot_training_results(history, f'finetune_{APPROACH_NAME}.png')\n",
    "model.encoder.save_weights(f'finetune_{APPROACH_NAME}_enc.h5')\n",
    "model.decoder.save_weights(f'finetune_{APPROACH_NAME}_dec.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSAljxKY18vC"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CE7jIqCl18vC"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(imagenet_model, imagenet_output_layer)\n",
    "decoder = Decoder(encoder.outputs[0].shape[1:])\n",
    "encoder.load_weights(f'Fine-tuning/finetune_{APPROACH_NAME}_enc.h5')\n",
    "decoder.load_weights(f'Fine-tuning/finetune_{APPROACH_NAME}_dec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRddbIUi18vD"
   },
   "outputs": [],
   "source": [
    "reset_model = EncoderDecoderModel(encoder, decoder, data_handler)\n",
    "reset_model.compile(\n",
    "    optimizer = Adam(LEARNING_RATE), \n",
    "    loss = MaskedLoss(), \n",
    "    metrics = [\n",
    "        SequenceAccuracy(),\n",
    "        CharacterAccuracy(),\n",
    "        LevenshteinDistance(normalize=True, name='lev_distance')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1UkLQIT18vD"
   },
   "source": [
    "## On test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzfgGHNf18vD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "for idx, (batch_images, batch_tokens) in enumerate(valid_tf_dataset.take(1)):\n",
    "    idxs_in_batch = valid_idxs[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE]\n",
    "    labels = data_handler.tokens2texts(batch_tokens)\n",
    "    pred_tokens, attentions = reset_model.predict(batch_images, return_attention=True)\n",
    "    pred_labels = data_handler.tokens2texts(pred_tokens)\n",
    "    \n",
    "    batch_results.append({'true': labels, 'pred': pred_labels, 'attentions': attentions})\n",
    "    visualize_images_labels(\n",
    "        img_paths = dataset.img_paths[idxs_in_batch], \n",
    "        labels = labels, \n",
    "        pred_labels = pred_labels,\n",
    "        figsize = (11.6, 30),\n",
    "        subplot_size = (4, 8),\n",
    "        legend_loc = (3.8, 4.38),\n",
    "        annotate_loc = (4, 2.75),\n",
    "        font_path = FONT_PATH, \n",
    "    )\n",
    "    print(\n",
    "        f'Batch {idx + 1:02d}:\\n'\n",
    "        f'- True: {dict(enumerate(labels, start=1))}\\n'\n",
    "        f'- Pred: {dict(enumerate(pred_labels, start=1))}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLRmJs-N18vD"
   },
   "source": [
    "## On random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TumY4TEf18vD"
   },
   "outputs": [],
   "source": [
    "random_path = '../囷𦝄苔惮󰞺𧍋𦬑囊.jpg'\n",
    "random_label = '囷𦝄苔惮󰞺𧍋𦬑囊'\n",
    "random_image = data_handler.process_image(random_path)\n",
    "pred_tokens = reset_model.predict(tf.expand_dims(random_image, axis=0))\n",
    "pred_labels = data_handler.tokens2texts(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNVbsf5mRycj"
   },
   "outputs": [],
   "source": [
    "visualize_images_labels(\n",
    "    img_paths = [random_path], \n",
    "    labels = [random_label], \n",
    "    pred_labels = pred_labels,\n",
    "    figsize = (5, 4),\n",
    "    subplot_size = (1, 1), \n",
    "    font_path = FONT_PATH, \n",
    ")\n",
    "print('Predicted text:', ''.join(pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluator import Evaluator\n",
    "GT10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_gt10.txt'\n",
    "LTE10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_lte10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt10_evaluator = Evaluator(reset_model, DATASET_DIR, GT10_TRANSCRIPTS_PATH)\n",
    "lte10_evaluator = Evaluator(reset_model, DATASET_DIR, LTE10_TRANSCRIPTS_PATH)\n",
    "df = pd.DataFrame([\n",
    "    reset_model.evaluate(valid_tf_dataset, return_dict=True),\n",
    "    gt10_evaluator.evaluate(data_handler, BATCH_SIZE, drop_remainder=True),\n",
    "    lte10_evaluator.evaluate(data_handler, BATCH_SIZE, drop_remainder=True),\n",
    "])\n",
    "df.index = ['Full', 'Length > 10', 'Length ≤ 10']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ImageCaptioning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71a7bf2136a97577d0a8690417094bf6019d7ad150fe8630a15825b0bcf133e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
