{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "670dNtc0LR0z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/transformer\n",
    "# https://keras.io/examples/nlp/neural_machine_translation_with_transformer\n",
    "# https://keras.io/examples/vision/image_captioning\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "APPROACH_NAME = 'CNNxTransformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL95XLRnDLr0"
   },
   "source": [
    "# Check GPU working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC47NIx6C8tx",
    "outputId": "4a69b76c-3ea0-46f2-f8cf-be6798d6d6c6"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0': raise SystemError('GPU device not found')\n",
    "print('Found GPU at:', device_name)\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0RvhBn63BXh"
   },
   "source": [
    "# Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7sM_I-v3BXi"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = r'../../Datasets/Patches'\n",
    "ALL_TRANSCRIPTS_PATH = f'{DATASET_DIR}/All.txt'\n",
    "VALID_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate.txt'\n",
    "FONT_PATH = r'../../NomNaTong-Regular.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9lAUxB63BXj"
   },
   "source": [
    "## Load and remove records with rare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYzNVfQU18u7",
    "outputId": "b47a887e-c105-4d03-9eb6-ca3308ba2c98",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from loader import DataImporter\n",
    "dataset = DataImporter(DATASET_DIR, ALL_TRANSCRIPTS_PATH, min_length=1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ufvmp3v18u7"
   },
   "source": [
    "## Data constants and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EvxKf_W18u8"
   },
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 432, 48\n",
    "PADDING_CHAR = '[PAD]' \n",
    "START_CHAR = '[START]'\n",
    "END_CHAR = '[END]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buqQdwSa18u8"
   },
   "outputs": [],
   "source": [
    "from loader import DataHandler\n",
    "data_handler = DataHandler(\n",
    "    dataset, \n",
    "    img_size = (HEIGHT, WIDTH), \n",
    "    padding_char = PADDING_CHAR,\n",
    "    start_char = START_CHAR,\n",
    "    end_char = END_CHAR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN5vVmTP18u9"
   },
   "outputs": [],
   "source": [
    "NUM_VALIDATE = DataImporter(DATASET_DIR, VALID_TRANSCRIPTS_PATH, min_length=1).size\n",
    "MAX_LENGTH = data_handler.max_length\n",
    "START_TOKEN = data_handler.start_token\n",
    "END_TOKEN = data_handler.end_token\n",
    "VOCAB_SIZE = data_handler.char2num.vocab_size()\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqe2peRS3BXr"
   },
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "S_vABuOD3BXs",
    "outputId": "4286fd14-bde2-4846-a42a-1753da92fa83"
   },
   "outputs": [],
   "source": [
    "from visualizer import visualize_images_labels\n",
    "visualize_images_labels(\n",
    "    dataset.img_paths, \n",
    "    dataset.labels, \n",
    "    figsize = (15, 15),\n",
    "    subplot_size = (2, 8),\n",
    "    font_path = FONT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from layers import custom_cnn, reshape_features\n",
    "from transformer import TransformerEncoderBlock, TransformerDecoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRfgDRNZ3BX2"
   },
   "outputs": [],
   "source": [
    "def get_cnn_model(imagenet_model=None, imagenet_output_layer=None, name='CNN_model'):\n",
    "    if imagenet_model: # Use Imagenet model as CNN layers\n",
    "        image_input = imagenet_model.input\n",
    "        imagenet_model.layers[0]._name = 'image'\n",
    "        x = imagenet_model.get_layer(imagenet_output_layer).output\n",
    "    else: \n",
    "        image_input = Input(shape=(HEIGHT, WIDTH, 3), dtype='float32', name='image')\n",
    "        conv_blocks_config = {\n",
    "            'block1': {'num_conv': 1, 'filters':  64, 'pool_size': (2, 2)}, \n",
    "            'block2': {'num_conv': 1, 'filters': 128, 'pool_size': (2, 2)}, \n",
    "            'block3': {'num_conv': 2, 'filters': 256, 'pool_size': (2, 2)}, \n",
    "            'block4': {'num_conv': 2, 'filters': 512, 'pool_size': (2, 2)}, \n",
    "            \n",
    "            # Last Conv blocks with 2x2 kernel but without no padding and pooling layer\n",
    "            'block5': {'num_conv': 2, 'filters': 512, 'pool_size': None}, \n",
    "        }\n",
    "        x = custom_cnn(conv_blocks_config, image_input)\n",
    "    \n",
    "    # Reshape accordingly before passing output to the Transformer encoder\n",
    "    feature_maps = reshape_features(x, dim_to_keep=1)\n",
    "    return tf.keras.Model(inputs=image_input, outputs=feature_maps, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_imagenet_model\n",
    "imagenet_model, imagenet_output_layer = None, None\n",
    "# # Pick a model from https://keras.io/api/applications\n",
    "# imagenet_model = get_imagenet_model('VGG16', (HEIGHT, WIDTH, 3))\n",
    "# imagenet_output_layer = 'block5_pool'\n",
    "# imagenet_model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 1\n",
    "EMBEDDING_DIM = 512 # d_model\n",
    "FEED_FORWARD_UNITS = 512 # dff\n",
    "DROPOUT_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model(imagenet_model, imagenet_output_layer)\n",
    "encoder = TransformerEncoderBlock(\n",
    "    receptive_size = cnn_model.layers[-1].output_shape[-2],\n",
    "    num_layers = NUM_LAYERS, # N encoder layers\n",
    "    num_heads = NUM_HEADS,\n",
    "    embedding_dim = EMBEDDING_DIM, # d_model\n",
    "    feed_forward_units = FEED_FORWARD_UNITS, # dff\n",
    "    dropout_rate = DROPOUT_RATE,\n",
    ")\n",
    "decoder = TransformerDecoderBlock(\n",
    "    enc_shape = encoder.output_shape[1:], # (receptive_size, enc_channels)\n",
    "    seq_length = MAX_LENGTH - 1, # The inputs is shifted by 1\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    num_layers = NUM_LAYERS, # N decoder layers\n",
    "    num_heads = NUM_HEADS,\n",
    "    embedding_dim = EMBEDDING_DIM, # d_model\n",
    "    feed_forward_units = FEED_FORWARD_UNITS, # dff\n",
    "    dropout_rate = DROPOUT_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_weights(f'./IHR-NomDB/IHR-NomDB_{APPROACH_NAME}_cnn.h5')\n",
    "encoder.load_weights(f'./IHR-NomDB/IHR-NomDB_{APPROACH_NAME}_enc.h5')\n",
    "decoder.load_weights(\n",
    "    f'./IHR-NomDB/IHR-NomDB_{APPROACH_NAME}_dec.h5', \n",
    "    skip_mismatch = True,\n",
    "    by_name = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformer import TransformerOCR\n",
    "model = TransformerOCR(cnn_model, encoder, decoder, data_handler)\n",
    "cnn_model.summary(line_length=90)\n",
    "print()\n",
    "encoder.summary(line_length=110)\n",
    "print()\n",
    "decoder.summary(line_length=110)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjT21l-13BX-"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJa9IM6wePoK",
    "outputId": "8569e982-5986-48a5-bf69-6de58d77fe2c"
   },
   "outputs": [],
   "source": [
    "train_idxs = list(range(dataset.size - NUM_VALIDATE))\n",
    "valid_idxs = list(range(train_idxs[-1] + 1, dataset.size))\n",
    "print('Number of training samples:', len(train_idxs))\n",
    "print('Number of validate samples:', len(valid_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "375pZk8K18vB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2022)\n",
    "random.shuffle(train_idxs)\n",
    "\n",
    "# When run on a small RAM machine, you can set use_cache=False to \n",
    "# not run out of memory but it will slow down the training speed\n",
    "train_tf_dataset = data_handler.prepare_tf_dataset(train_idxs, BATCH_SIZE, drop_remainder=True)\n",
    "valid_tf_dataset = data_handler.prepare_tf_dataset(valid_idxs, BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    min_delta = 1e-3, # Change that less than 1e-3, will count as no improvement\n",
    "    patience = 5, # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights = True, # Restore weights from the epoch with the best value\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sh1CdC18vB"
   },
   "source": [
    "## Fine-tuning the NomNaOCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import MaskedLoss\n",
    "from metrics import SequenceAccuracy, CharacterAccuracy, LevenshteinDistance\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "# Adadelta tends to benefit from higher initial learning rate values compared to\n",
    "# other optimizers. Here use 1.0 to match the exact form in the original paper\n",
    "LEARNING_RATE = 1.0\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPVb3axr18vC"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adadelta(LEARNING_RATE), \n",
    "    loss = MaskedLoss(), \n",
    "    metrics = [\n",
    "        SequenceAccuracy(),\n",
    "        CharacterAccuracy(),\n",
    "        LevenshteinDistance(normalize=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiWc6i5B3BYB",
    "outputId": "79cc561c-f6ec-42b2-b549-3f5a3713e4d0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data = valid_tf_dataset,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [early_stopping_callback],\n",
    "    verbose = 1\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jU-5S6vo18vC"
   },
   "source": [
    "## Save the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtA0QVnz18vC"
   },
   "outputs": [],
   "source": [
    "best_epoch = early_stopping_callback.best_epoch\n",
    "print(f'- Loss on validation\\t: {history[\"val_loss\"][best_epoch]}')\n",
    "print(f'- Sequence accuracy\\t: {history[\"val_seq_acc\"][best_epoch]}')\n",
    "print(f'- Character accuracy\\t: {history[\"val_char_acc\"][best_epoch]}')\n",
    "print(f'- Levenshtein distance\\t: {history[\"val_levenshtein_distance\"][best_epoch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5MgFd3a18vC"
   },
   "outputs": [],
   "source": [
    "from visualizer import plot_training_results\n",
    "plot_training_results(history, f'finetune_{APPROACH_NAME}.png')\n",
    "model.cnn_model.save_weights(f'finetune_{APPROACH_NAME}_cnn.h5')\n",
    "model.encoder.save_weights(f'finetune_{APPROACH_NAME}_enc.h5')\n",
    "model.decoder.save_weights(f'finetune_{APPROACH_NAME}_dec.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSAljxKY18vC"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_cnn_model(imagenet_model, imagenet_output_layer)\n",
    "encoder = TransformerEncoderBlock(\n",
    "    receptive_size = cnn_model.layers[-1].output_shape[-2],\n",
    "    num_layers = NUM_LAYERS, # N encoder layers\n",
    "    num_heads = NUM_HEADS,\n",
    "    embedding_dim = EMBEDDING_DIM, # d_model\n",
    "    feed_forward_units = FEED_FORWARD_UNITS, # dff\n",
    "    dropout_rate = DROPOUT_RATE,\n",
    ")\n",
    "decoder = TransformerDecoderBlock(\n",
    "    enc_shape = encoder.output_shape[1:], # (receptive_size, enc_channels)\n",
    "    seq_length = MAX_LENGTH - 1, # The inputs is shifted by 1\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    num_layers = NUM_LAYERS, # N decoder layers\n",
    "    num_heads = NUM_HEADS,\n",
    "    embedding_dim = EMBEDDING_DIM, # d_model\n",
    "    feed_forward_units = FEED_FORWARD_UNITS, # dff\n",
    "    dropout_rate = DROPOUT_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_weights(f'Fine-tuning/finetune_{APPROACH_NAME}_cnn.h5')\n",
    "encoder.load_weights(f'Fine-tuning/finetune_{APPROACH_NAME}_enc.h5')\n",
    "decoder.load_weights(f'Fine-tuning/finetune_{APPROACH_NAME}_dec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRddbIUi18vD"
   },
   "outputs": [],
   "source": [
    "reset_model = TransformerOCR(cnn_model, encoder, decoder, data_handler)\n",
    "reset_model.compile(\n",
    "    optimizer = Adadelta(LEARNING_RATE), \n",
    "    loss = MaskedLoss(), \n",
    "    metrics = [\n",
    "        SequenceAccuracy(),\n",
    "        CharacterAccuracy(),\n",
    "        LevenshteinDistance(normalize=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1UkLQIT18vD"
   },
   "source": [
    "## On test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzfgGHNf18vD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "for idx, (batch_images, batch_tokens) in enumerate(valid_tf_dataset.take(1)):\n",
    "    idxs_in_batch = valid_idxs[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE]\n",
    "    labels = data_handler.tokens2texts(batch_tokens)\n",
    "    pred_tokens, attentions = reset_model.predict(batch_images, return_attention=True)\n",
    "    pred_labels = data_handler.tokens2texts(pred_tokens)\n",
    "    \n",
    "    batch_results.append({'true': labels, 'pred': pred_labels, 'attentions': attentions})\n",
    "    visualize_images_labels(\n",
    "        img_paths = dataset.img_paths[idxs_in_batch], \n",
    "        labels = labels, \n",
    "        pred_labels = pred_labels,\n",
    "        figsize = (11.6, 30),\n",
    "        subplot_size = (4, 8),\n",
    "        legend_loc = (3.8, 4.38),\n",
    "        annotate_loc = (4, 2.75),\n",
    "        font_path = FONT_PATH, \n",
    "    )\n",
    "    print(\n",
    "        f'Batch {idx + 1:02d}:\\n'\n",
    "        f'- True: {dict(enumerate(labels, start=1))}\\n'\n",
    "        f'- Pred: {dict(enumerate(pred_labels, start=1))}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLRmJs-N18vD"
   },
   "source": [
    "## On random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TumY4TEf18vD"
   },
   "outputs": [],
   "source": [
    "random_path = '../囷𦝄苔惮󰞺𧍋𦬑囊.jpg'\n",
    "random_label = '囷𦝄苔惮󰞺𧍋𦬑囊'\n",
    "random_image = data_handler.process_image(random_path)\n",
    "pred_tokens = reset_model.predict(tf.expand_dims(random_image, axis=0))\n",
    "pred_labels = data_handler.tokens2texts(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNVbsf5mRycj"
   },
   "outputs": [],
   "source": [
    "visualize_images_labels(\n",
    "    img_paths = [random_path], \n",
    "    labels = [random_label], \n",
    "    pred_labels = pred_labels,\n",
    "    figsize = (5, 4),\n",
    "    subplot_size = (1, 1), \n",
    "    font_path = FONT_PATH, \n",
    ")\n",
    "print('Predicted text:', ''.join(pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluator import Evaluator\n",
    "GT10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_gt10.txt'\n",
    "LTE10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_lte10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt10_evaluator = Evaluator(reset_model, DATASET_DIR, GT10_TRANSCRIPTS_PATH)\n",
    "lte10_evaluator = Evaluator(reset_model, DATASET_DIR, LTE10_TRANSCRIPTS_PATH)\n",
    "df = pd.DataFrame([\n",
    "    reset_model.evaluate(valid_tf_dataset, return_dict=True),\n",
    "    gt10_evaluator.evaluate(data_handler, BATCH_SIZE, drop_remainder=True),\n",
    "    lte10_evaluator.evaluate(data_handler, BATCH_SIZE, drop_remainder=True),\n",
    "])\n",
    "df.index = ['Full', 'Length > 10', 'Length ≤ 10']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Injection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
